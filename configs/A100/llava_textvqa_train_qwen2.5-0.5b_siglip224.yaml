project:
  name: llena
paths:
  artifacts_dir: artifacts
  reports_dir: reports
  cache_dir: ${HF_HOME:-.cache/huggingface}
model:
  llm_name: Qwen/Qwen2.5-1.5B-Instruct
  vision_name: google/siglip-large-patch16-384
  llm_revision: null
  vision_revision: null
  attn_implementation: flash_attention_2
mm:
  projector: mlp2
train:
  seed: 42
  device: cuda
  precision: bf16
  gradient_checkpointing: true
  weight_decay: 0.0
  warmup_ratio: 0.03
  lr_schedule: cosine
  max_grad_norm: 0.5
  max_seq_len: 2048
  epochs: 1
  batch_size: 128
  micro_batch_size: 16
  num_workers: 4
  pin_memory: true
  persistent_workers: false
  log_every: 10
  eval_every: 200
  save_every: 200
  save_total_limit: 3
  max_steps: null
  stage:
    name: lora
    params:
      lora_r: 128
      lora_alpha: 256
      lora_dropout: 0.05
      lora_targets:
        - q_proj
        - k_proj
        - v_proj
        - o_proj
        - gate_proj
        - up_proj
        - down_proj
  lr: 0.0002
data:
  dataset: llava_textvqa
  data_dir: datasets/processed
  split: train
logging:
  backend: wandb
  wandb_project: llena
eval:
  mode: teacher
  batch_size: 8
  num_workers: 4
  pin_memory: true
  persistent_workers: false
  max_generated_tokens: 256
