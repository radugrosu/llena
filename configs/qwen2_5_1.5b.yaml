# configs/qwen2_5_3b.yaml
project:
  name: llena
  run_name: "qwen2.5-1.5b_siglip_mlp2_smoke"

paths:
  artifacts_dir: "artifacts"
  reports_dir: "reports"
  cache_dir: "${HF_HOME:-.cache/huggingface}"

model:
  llm_name: "Qwen/Qwen2.5-1.5B-Instruct"
  vision_name: "google/siglip-base-patch16-224"

  # Optional: HF revision pins for reproducibility
  llm_revision: null
  vision_revision: null

mm:
  # Number of "virtual image tokens" to prepend (after projection)
  num_image_tokens: 256

  # "mlp2" or "perceiver" (perceiver can come later)
  projector: "mlp2"

  # For perceiver/resampler (ignored for mlp2)
  perceiver_latents: 64
  perceiver_layers: 2

train:
  seed: 42
  device: "auto" # "auto" | "cuda" | "cpu"
  precision: "bf16" # "bf16" | "fp16" | "fp32"
  gradient_checkpointing: true

  # LoRA / QLoRA knobs
  use_lora: true
  use_qlora: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  lora_targets: ["q_proj", "k_proj", "v_proj", "o_proj"]

  # Optimization
  lr_lora: 1.0e-4
  lr_projector: 2.0e-4
  weight_decay: 0.0
  warmup_ratio: 0.03
  max_grad_norm: 1.0

  # Sequence & batching
  max_seq_len: 1024
  micro_batch_size: 1
  grad_accum_steps: 4

  # Logging/checkpointing
  log_every: 10
  eval_every: 0 # 0 disables periodic eval for smoke runs
  save_every: 100
  save_total_limit: 2

data:
  # For stage-0 smoke, use a tiny synthetic dataset
  dataset: "synthetic"
  num_samples: 64
  image_size: 224

eval:
  enabled: true
  max_samples: 32

logging:
  backend: "wandb" # "none" | "wandb" | "mlflow"
  wandb_project: null
