{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llena inspection notebook\n",
    "\n",
    "Quick, small-scale inspection of datasets, tokenizer, model inputs/outputs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Optional: install deps in Colab\n",
    "# !pip -q install torch torchvision transformers datasets peft bitsandbytes pillow tqdm wandb\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "# If running from notebooks/, add repo root to sys.path\n",
    "repo_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print('repo_root:', repo_root)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a small dataset slice\n",
    "\n",
    "This uses a small validation slice to keep it fast."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Note: HF may still download parquet shards even if you take a small slice.\n",
    "# We load the split and then select a small subset for inspection.\n",
    "ds_full = load_dataset('lmms-lab/textvqa', split='validation')\n",
    "ds = ds_full.select(range(50))\n",
    "print(ds)\n",
    "sample = ds[0]\n",
    "list(sample.keys())\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Inspect a sample\n",
    "sample\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Llena model + collator\n",
    "\n",
    "Use a small config and run a single forward pass."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from mm.model import LlenaModel, LlenaModelConfig\n",
    "from mm.collator import LlenaCollator\n",
    "from transformers import SiglipImageProcessor\n",
    "\n",
    "cfg = LlenaModelConfig(\n",
    "    llm_name='Qwen/Qwen2.5-0.5B-Instruct',\n",
    "    vision_name='google/siglip-base-patch16-224',\n",
    "    num_image_tokens=64,\n",
    "    projector='mlp2',\n",
    "    freeze_vision=True,\n",
    "    freeze_llm=True,\n",
    "    gradient_checkpointing=False,\n",
    "    device='cuda' if device.type == 'cuda' else 'cpu',\n",
    ")\n",
    "model = LlenaModel(cfg)\n",
    "model.eval()\n",
    "\n",
    "image_proc = SiglipImageProcessor.from_pretrained(cfg.vision_name)\n",
    "collator = LlenaCollator(\n",
    "    tokenizer=model.tokenizer,\n",
    "    image_processor=image_proc,\n",
    "    max_seq_len=128,\n",
    "    num_image_tokens=cfg.num_image_tokens,\n",
    "    pad_to_multiple_of=None,\n",
    ")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build a small batch from the HF dataset\n",
    "# Convert HF sample to VQASample format\n",
    "from PIL import Image\n",
    "\n",
    "def to_vqa(sample):\n",
    "    return {\n",
    "        'image': sample['image'],\n",
    "        'question': sample['question'],\n",
    "        'answer': sample['answers'][0],\n",
    "        'answers': sample['answers'],\n",
    "    }\n",
    "\n",
    "batch = [to_vqa(ds[i]) for i in range(2)]\n",
    "out = collator(batch)\n",
    "batch_t = {k: v.to(device) for k, v in out.items() if torch.is_tensor(v)}\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        pixel_values=batch_t['pixel_values'],\n",
    "        input_ids=batch_t['input_ids'],\n",
    "        mm_attention_mask=batch_t['mm_attention_mask'],\n",
    "        mm_labels=batch_t['mm_labels'],\n",
    "    )\n",
    "\n",
    "outputs.loss, outputs.logits.shape\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Decode the model's argmax tokens for the answer region (quick sanity check)\n",
    "logits = outputs.logits\n",
    "pred_ids = logits.argmax(dim=-1)\n",
    "mask = batch_t['mm_labels'][0] != -100\n",
    "pred_seq = pred_ids[0][mask].tolist()\n",
    "model.tokenizer.decode(pred_seq, skip_special_tokens=True)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: inspect processed JSONL dataset\n",
    "\n",
    "If you have processed data under `datasets/processed`, you can load it with JsonlVQADataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from data.format import JsonlVQADataset\n",
    "\n",
    "proc_root = repo_root / 'datasets' / 'processed' / 'textvqa'\n",
    "jsonl = proc_root / 'validation.jsonl'\n",
    "images = proc_root / 'images'\n",
    "if jsonl.exists():\n",
    "    ds_jsonl = JsonlVQADataset(annotations_path=jsonl, image_root=images, max_samples=5)\n",
    "    print('jsonl samples:', len(ds_jsonl))\n",
    "    print(ds_jsonl[0])\n",
    "else:\n",
    "    print('No processed JSONL found at', jsonl)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flash attention check\n",
    "\n",
    "Set a config path and device, then verify `attn_implementation`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "CFG_PATH = \"configs/L4/sharegpt4v_train_qwen2.5-0.5b_siglip224.yaml\"\n",
    "DEVICE = \"cuda\"  # or \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from mm.config import load_config\n",
    "from mm.run_config import RunConfig\n",
    "\n",
    "cfg = load_config(CFG_PATH)\n",
    "rc = RunConfig.from_dict(cfg)\n",
    "print(\"attn_implementation:\", rc.model.attn_implementation)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from mm.model import LlenaModel, LlenaModelConfig\n",
    "\n",
    "if DEVICE == \"cuda\" and not torch.cuda.is_available():\n",
    "    print(\"CUDA not available; skipping model load.\")\n",
    "else:\n",
    "    mcfg = LlenaModelConfig(\n",
    "        llm_name=rc.model.llm_name,\n",
    "        vision_name=rc.model.vision_name,\n",
    "        num_image_tokens=rc.mm.num_image_tokens,\n",
    "        projector=rc.mm.projector,\n",
    "        device=DEVICE,\n",
    "        gradient_checkpointing=rc.train.gradient_checkpointing,\n",
    "        peft_enable=False,\n",
    "        qlora_enable=False,\n",
    "        attn_implementation=rc.model.attn_implementation,\n",
    "    )\n",
    "    model = LlenaModel(mcfg)\n",
    "    print(\"Loaded model with attn_implementation:\", rc.model.attn_implementation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug generation (empty outputs)\n",
    "Use this to test a single image + prompt and inspect raw generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import SiglipImageProcessor\n",
    "from mm.model import LlenaModel, LlenaModelConfig, _select_or_pad_tokens\n",
    "\n",
    "CKPT_PATH = 'artifacts/your_run/step_xxx/ckpt.pt'  # TODO\n",
    "IMAGE_PATH = 'datasets/processed/sharegpt4v_coco/images/000000000073.jpg'  # TODO\n",
    "QUESTION = 'what is the license plate of this vehicle'\n",
    "MAX_GENERATED_TOKENS = 256\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
    "cfg_d = ckpt.get('cfg')\n",
    "if not isinstance(cfg_d, dict):\n",
    "    raise ValueError('ckpt missing cfg')\n",
    "\n",
    "cfg = LlenaModelConfig(\n",
    "    llm_name=cfg_d['llm_name'],\n",
    "    vision_name=cfg_d['vision_name'],\n",
    "    num_image_tokens=cfg_d['num_image_tokens'],\n",
    "    projector=cfg_d['projector'],\n",
    "    gradient_checkpointing=False,\n",
    "    freeze_vision=True,\n",
    "    freeze_llm=True,\n",
    "    peft_enable='adapter' in ckpt,\n",
    "    peft_r=0,\n",
    "    peft_alpha=0,\n",
    "    peft_dropout=0.0,\n",
    "    peft_target_modules=[],\n",
    "    qlora_enable=False,\n",
    "    device='cuda' if device.type == 'cuda' else 'cpu',\n",
    ")\n",
    "model = LlenaModel(cfg)\n",
    "model.eval()\n",
    "if 'model' in ckpt:\n",
    "    model.load_state_dict(ckpt['model'], strict=True)\n",
    "if 'projector' in ckpt:\n",
    "    model.projector.load_state_dict(ckpt['projector'], strict=True)\n",
    "if 'adapter' in ckpt:\n",
    "    from peft import set_peft_model_state_dict\n",
    "    set_peft_model_state_dict(model.llm, ckpt['adapter'])\n",
    "\n",
    "image_proc = SiglipImageProcessor.from_pretrained(cfg.vision_name)\n",
    "img = Image.open(IMAGE_PATH).convert('RGB')\n",
    "vision = image_proc(images=[img], return_tensors='pt')\n",
    "pixel_values = vision['pixel_values'].to(device)\n",
    "\n",
    "tokenizer = model.tokenizer\n",
    "pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "prompt_ids = tokenizer.apply_chat_template(\n",
    "    [{'role': 'user', 'content': QUESTION}],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_tensors=None,\n",
    ")\n",
    "input_ids = torch.tensor([prompt_ids], device=device)\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "with torch.no_grad():\n",
    "    v_out = model.vision(pixel_values=pixel_values)\n",
    "    v_tokens = v_out.last_hidden_state\n",
    "    if model.freeze_vision:\n",
    "        v_tokens = v_tokens.detach()\n",
    "    v_tokens = _select_or_pad_tokens(v_tokens, model.num_image_tokens)\n",
    "    proj_dtype = next(model.projector.parameters()).dtype\n",
    "    v_tokens = v_tokens.to(dtype=proj_dtype)\n",
    "    img_embeds = model.projector(v_tokens)\n",
    "\n",
    "    text_embeds = model.llm.get_input_embeddings()(input_ids)\n",
    "    if img_embeds.dtype != text_embeds.dtype:\n",
    "        img_embeds = img_embeds.to(dtype=text_embeds.dtype)\n",
    "    inputs_embeds = torch.cat([img_embeds, text_embeds], dim=1)\n",
    "\n",
    "    prefix_mask = torch.ones((attention_mask.size(0), model.num_image_tokens), dtype=attention_mask.dtype, device=attention_mask.device)\n",
    "    mm_attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
    "\n",
    "    gen_ids = model.llm.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=mm_attention_mask,\n",
    "        do_sample=False,\n",
    "        num_beams=1,\n",
    "        max_new_tokens=MAX_GENERATED_TOKENS,\n",
    "        temperature=0.0,\n",
    "        repetition_penalty=1.0,\n",
    "        pad_token_id=pad_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "prefix_len = inputs_embeds.size(1)\n",
    "gen_new = gen_ids[:, prefix_len:]\n",
    "print('prompt tokens:', len(prompt_ids))\n",
    "print('generated tokens:', gen_new.size(1))\n",
    "print('response:', tokenizer.batch_decode(gen_new.tolist(), skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug generation from eval samples\n",
    "Loads a sample from `datasets/samples.json` and runs a single-step generation for inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import SiglipImageProcessor\n",
    "from mm.model import LlenaModel, LlenaModelConfig, _select_or_pad_tokens\n",
    "\n",
    "SAMPLES_JSON = 'datasets/samples.json'  # TODO\n",
    "SAMPLE_INDEX = 0\n",
    "CKPT_PATH = 'artifacts/your_run/step_xxx/ckpt.pt'  # TODO\n",
    "MAX_GENERATED_TOKENS = 256\n",
    "\n",
    "sample = json.loads(Path(SAMPLES_JSON).read_text(encoding='utf-8'))[SAMPLE_INDEX]\n",
    "IMAGE_PATH = sample.get('image_path') or sample.get('image')\n",
    "QUESTION = sample.get('question')\n",
    "if not IMAGE_PATH or not QUESTION:\n",
    "    raise ValueError('Sample must include image_path and question')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
    "cfg_d = ckpt.get('cfg')\n",
    "if not isinstance(cfg_d, dict):\n",
    "    raise ValueError('ckpt missing cfg')\n",
    "\n",
    "cfg = LlenaModelConfig(\n",
    "    llm_name=cfg_d['llm_name'],\n",
    "    vision_name=cfg_d['vision_name'],\n",
    "    num_image_tokens=cfg_d['num_image_tokens'],\n",
    "    projector=cfg_d['projector'],\n",
    "    gradient_checkpointing=False,\n",
    "    freeze_vision=True,\n",
    "    freeze_llm=True,\n",
    "    peft_enable='adapter' in ckpt,\n",
    "    peft_r=0,\n",
    "    peft_alpha=0,\n",
    "    peft_dropout=0.0,\n",
    "    peft_target_modules=[],\n",
    "    qlora_enable=False,\n",
    "    device='cuda' if device.type == 'cuda' else 'cpu',\n",
    ")\n",
    "model = LlenaModel(cfg)\n",
    "model.eval()\n",
    "if 'model' in ckpt:\n",
    "    model.load_state_dict(ckpt['model'], strict=True)\n",
    "if 'projector' in ckpt:\n",
    "    model.projector.load_state_dict(ckpt['projector'], strict=True)\n",
    "if 'adapter' in ckpt:\n",
    "    from peft import set_peft_model_state_dict\n",
    "    set_peft_model_state_dict(model.llm, ckpt['adapter'])\n",
    "\n",
    "image_proc = SiglipImageProcessor.from_pretrained(cfg.vision_name)\n",
    "img = Image.open(IMAGE_PATH).convert('RGB')\n",
    "vision = image_proc(images=[img], return_tensors='pt')\n",
    "pixel_values = vision['pixel_values'].to(device)\n",
    "\n",
    "tokenizer = model.tokenizer\n",
    "pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "prompt_ids = tokenizer.apply_chat_template(\n",
    "    [{'role': 'user', 'content': QUESTION}],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_tensors=None,\n",
    ")\n",
    "input_ids = torch.tensor([prompt_ids], device=device)\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "with torch.no_grad():\n",
    "    v_out = model.vision(pixel_values=pixel_values)\n",
    "    v_tokens = v_out.last_hidden_state\n",
    "    if model.freeze_vision:\n",
    "        v_tokens = v_tokens.detach()\n",
    "    v_tokens = _select_or_pad_tokens(v_tokens, model.num_image_tokens)\n",
    "    proj_dtype = next(model.projector.parameters()).dtype\n",
    "    v_tokens = v_tokens.to(dtype=proj_dtype)\n",
    "    img_embeds = model.projector(v_tokens)\n",
    "\n",
    "    text_embeds = model.llm.get_input_embeddings()(input_ids)\n",
    "    if img_embeds.dtype != text_embeds.dtype:\n",
    "        img_embeds = img_embeds.to(dtype=text_embeds.dtype)\n",
    "    inputs_embeds = torch.cat([img_embeds, text_embeds], dim=1)\n",
    "\n",
    "    prefix_mask = torch.ones((attention_mask.size(0), model.num_image_tokens), dtype=attention_mask.dtype, device=attention_mask.device)\n",
    "    mm_attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
    "\n",
    "    gen_ids = model.llm.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=mm_attention_mask,\n",
    "        do_sample=False,\n",
    "        num_beams=1,\n",
    "        max_new_tokens=MAX_GENERATED_TOKENS,\n",
    "        temperature=0.0,\n",
    "        repetition_penalty=1.0,\n",
    "        pad_token_id=pad_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "prefix_len = inputs_embeds.size(1)\n",
    "gen_new = gen_ids[:, prefix_len:]\n",
    "print('image_path:', IMAGE_PATH)\n",
    "print('question:', QUESTION)\n",
    "print('prompt tokens:', len(prompt_ids))\n",
    "print('generated tokens:', gen_new.size(1))\n",
    "print('response:', tokenizer.batch_decode(gen_new.tolist(), skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download COCO image (if missing)\n",
    "Fetches a COCO 2017 train image by filename into a local folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "COCO_IMAGE = '000000000073.jpg'  # TODO\n",
    "DEST_DIR = 'datasets/processed/sharegpt4v_coco/images'\n",
    "\n",
    "dest = Path(DEST_DIR) / COCO_IMAGE\n",
    "dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "if not dest.exists():\n",
    "    url = f'http://images.cocodataset.org/train2017/{COCO_IMAGE}'\n",
    "    print('downloading', url)\n",
    "    urllib.request.urlretrieve(url, dest)\n",
    "    print('saved', dest)\n",
    "else:\n",
    "    print('already exists', dest)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}