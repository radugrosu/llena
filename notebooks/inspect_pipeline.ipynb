{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Llena inspection notebook\n",
        "\n",
        "Quick, small-scale inspection of datasets, tokenizer, model inputs/outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional: install deps in Colab\n",
        "# !pip -q install torch torchvision transformers datasets peft bitsandbytes pillow tqdm wandb\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "# If running from notebooks/, add repo root to sys.path\n",
        "repo_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
        "sys.path.insert(0, str(repo_root))\n",
        "\n",
        "print('repo_root:', repo_root)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device:', device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load a small dataset slice\n",
        "\n",
        "This uses a small validation slice to keep it fast."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ds = load_dataset('lmms-lab/textvqa', split='validation[:50]')\n",
        "print(ds)\n",
        "sample = ds[0]\n",
        "list(sample.keys())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inspect a sample\n",
        "sample\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Llena model + collator\n",
        "\n",
        "Use a small config and run a single forward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from mm.model import LlenaModel, LlenaModelConfig\n",
        "from mm.collator import LlenaCollator\n",
        "from transformers import SiglipImageProcessor\n",
        "\n",
        "cfg = LlenaModelConfig(\n",
        "    llm_name='Qwen/Qwen2.5-0.5B-Instruct',\n",
        "    vision_name='google/siglip-base-patch16-224',\n",
        "    num_image_tokens=64,\n",
        "    projector='mlp2',\n",
        "    freeze_vision=True,\n",
        "    freeze_llm=True,\n",
        "    gradient_checkpointing=False,\n",
        "    device='cuda' if device.type == 'cuda' else 'cpu',\n",
        ")\n",
        "model = LlenaModel(cfg)\n",
        "model.eval()\n",
        "\n",
        "image_proc = SiglipImageProcessor.from_pretrained(cfg.vision_name)\n",
        "collator = LlenaCollator(\n",
        "    tokenizer=model.tokenizer,\n",
        "    image_processor=image_proc,\n",
        "    max_seq_len=128,\n",
        "    num_image_tokens=cfg.num_image_tokens,\n",
        "    pad_to_multiple_of=None,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build a small batch from the HF dataset\n",
        "# Convert HF sample to VQASample format\n",
        "from PIL import Image\n",
        "\n",
        "def to_vqa(sample):\n",
        "    return {\n",
        "        'image': sample['image'],\n",
        "        'question': sample['question'],\n",
        "        'answer': sample['answers'][0],\n",
        "        'answers': sample['answers'],\n",
        "    }\n",
        "\n",
        "batch = [to_vqa(ds[i]) for i in range(2)]\n",
        "out = collator(batch)\n",
        "batch_t = {k: v.to(device) for k, v in out.items() if torch.is_tensor(v)}\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        pixel_values=batch_t['pixel_values'],\n",
        "        input_ids=batch_t['input_ids'],\n",
        "        mm_attention_mask=batch_t['mm_attention_mask'],\n",
        "        mm_labels=batch_t['mm_labels'],\n",
        "    )\n",
        "\n",
        "outputs.loss, outputs.logits.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Decode the model's argmax tokens for the answer region (quick sanity check)\n",
        "logits = outputs.logits\n",
        "pred_ids = logits.argmax(dim=-1)\n",
        "mask = batch_t['mm_labels'][0] != -100\n",
        "pred_seq = pred_ids[0][mask].tolist()\n",
        "model.tokenizer.decode(pred_seq, skip_special_tokens=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: inspect processed JSONL dataset\n",
        "\n",
        "If you have processed data under `datasets/processed`, you can load it with JsonlVQADataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from data.format import JsonlVQADataset\n",
        "\n",
        "proc_root = repo_root / 'datasets' / 'processed' / 'textvqa'\n",
        "jsonl = proc_root / 'validation.jsonl'\n",
        "images = proc_root / 'images'\n",
        "if jsonl.exists():\n",
        "    ds_jsonl = JsonlVQADataset(annotations_path=jsonl, image_root=images, max_samples=5)\n",
        "    print('jsonl samples:', len(ds_jsonl))\n",
        "    print(ds_jsonl[0])\n",
        "else:\n",
        "    print('No processed JSONL found at', jsonl)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}